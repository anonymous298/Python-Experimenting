{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Ollama LLM\n",
    "llm = ChatOllama(\n",
    "    model = 'deepseek-r1:1.5b'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', 'You are an helpful assistant'),\n",
    "        ('user', '{input}')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so I\\'m trying to understand what machine learning is. From the previous explanation, it seems like it\\'s about using algorithms to train computers on data to make predictions or decisions without being explicitly programmed. But I\\'m not entirely clear on all the details.\\n\\nLet me think about how this works. Suppose I want to predict whether a person will buy a product based on their past buying habits and other factors like age, gender, location, etc. How would machine learning help here? I guess it\\'s by taking all that data, maybe organize it into something called features or variables.\\n\\nThen, the algorithm might pick out the most important features, like if someone buys apples, they\\'re more likely to buy oranges too. That makes sense because fruits are often sold together. So the algorithm would learn from these patterns and improve its predictions over time as it gets more data.\\n\\nBut wait, how does this happen automatically? I mean, what happens when you give a computer all that data? The system has no instructions except through the algorithms. Maybe the algorithms find patterns on their own by trying different combinations of features and seeing which ones work best for making accurate predictions.\\n\\nI\\'m also curious about the difference between supervised and unsupervised learning. In supervised learning, I think it\\'s when you have labeled dataâ€”that is, data where each example is tagged with a label or outcome, like \"a person bought apples\" being labeled as \"1\". The algorithm uses these labels to learn from the patterns, so it can predict other outcomes based on new data.\\n\\nOn the other hand, unsupervised learning doesn\\'t require any labels. It\\'s used when you\\'re trying to find hidden structures in your data without knowing what to look for initially. For example, using clustering algorithms to group similar items together, like people with similar behaviors or preferences.\\n\\nBut then there are other types of machine learning too. I think there\\'s reinforcement learning where the algorithm learns through trial and error by performing actions and receiving rewards or feedback. It\\'s used in areas like game playing or robotics where it can learn from its interactions with the environment.\\n\\nWait, how does all this relate to real-world applications? I know that recommendation systems use machine learning to suggest products based on a user\\'s past behavior. Social media platforms might use clustering algorithms to group users together and provide personalized content. There are also services like Netflix or Amazon that use complex models to predict what you\\'ll watch next.\\n\\nBut how do these models actually make predictions? It seems like they take all the data, process it through some kind of algorithm, and then output a prediction. But how does this prediction become accurate over time? I guess with more data, the algorithms refine their patterns, improving their accuracy. Also, different datasets require different algorithms to perform well.\\n\\nI\\'m still a bit confused about how much manual intervention is needed. Since machine learning can find patterns on its own, maybe people don\\'t need to be as explicit in the features or labels they provide. It\\'s more about letting the data learn from itself.\\n\\nAnother thing I\\'m thinking about is interpretability. If someone is using machine learning models for decision-making, like loan approvals or medical diagnoses, it might be important whether those decisions are understandable and can explain how the model arrived at a particular conclusion. That could be a challenge because some machine learning models are \"black boxes,\" meaning their decisions aren\\'t transparent.\\n\\nBut I guess that\\'s not always the case. Many models do have interpretability features, especially when they\\'re used for critical applications. However, in general, as data becomes more complex and algorithms more powerful, transparency might become harder to achieve.\\n\\nOverall, machine learning seems like a broad field with many techniques and applications. It combines elements of statistics, computer science, and domain knowledge (like biology or finance). The goal is to enable systems to learn from data without being explicitly programmed, which allows them to make predictions or decisions with minimal human intervention.\\n</think>\\n\\nMachine Learning is a subset of artificial intelligence that involves algorithms enabling computers to learn and improve at tasks without explicit programming. It leverages patterns in data to make predictions or decisions. Here\\'s a structured overview:\\n\\n1. **Concept**: \\n   - Machine learning uses data (features) to train models, allowing them to predict outcomes based on unseen data.\\n   - Algorithms can identify important features, such as fruits being often sold together.\\n\\n2. **Types**:\\n   - **Supervised Learning**: Requires labeled data, where algorithms learn from examples with known outcomes (e.g., predicting if a person buys apples).\\n   - **Unsupervised Learning**: Identifies hidden structures in unlabeled data, like grouping users based on behavior.\\n   - **Reinforcement Learning**: learns through trial and error, useful in areas like game playing or robotics.\\n\\n3. **Applications**:\\n   - recommendation systems (e.g., Netflix) use clustering algorithms to group similar users.\\n   - Social media platforms offer personalized content based on user behavior patterns.\\n\\n4. **Learning Process**:\\n   - Algorithms process data, find patterns, and improve predictions over time with more data.\\n   - Data isn\\'t explicitly labeled; the system learns from examples without human intervention.\\n\\n5. **Challenges**:\\n   - Requires careful feature selection to ensure accurate predictions.\\n   - Interpretability may be a concern for black-box models, though many are transparent in critical applications.\\n\\nIn summary, machine learning enables systems to learn patterns from data and make predictions, with applications spanning finance, healthcare, and more. While it\\'s powerful, challenges include interpretability and the need for manual feature selection.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input' : \"what is Machine Learning\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
